= Rate Limiter

A fault-tolerance primitive that prevents clients of a system from exceeding a sustainable request rate.

Rate-limiters exist to protect services from unsustainable request volumes, such as during spikes in user traffic or denial-of-service attacks (including friendly-fire from misbehaving collaborator services) <<google-cloud>><<stripe>>. While services can utilize rate-limiting to protect themselves from high request volume, they can also use rate-limiting to control the volume of their own requests to unprotected legacy back-ends that are otherwise vulnerable <<google-cloud>>. In either case, rate-limiters deny requests above a certain throughput, shedding load off vulnerable systems, and may even prioritize certain types of requests over others to ensure less-significant systems degrade before more-critical systems <<stripe>>.

Clients of rate-limited systems should respond to rate-limiting intelligently or else risk https://landing.google.com/sre/sre-book/chapters/addressing-cascading-failures/[cascading failures] as errors from one service propagate uncontrolled throughout a network of collaborating services, possibly with self-reinforcing consequences <<google-cloud>>. In order for clients to respond to rate-limiting, services must signal clients when rate-limiting is active, such as by returning error code https://httpstatuses.com/429[429: Too Many Requests] <<getambassador>> during periods of high load. Clients will typically want to <<Retry,retry>>. As an alternative to rejecting excessive requests, services may be able to enqueue them for asynchronous fulfillment. In this scenario a service would immediately return a job ID to the client in response to their request. The client would know to poll the service or subscribe to an appropriate messaging channel for the response <<resilience4j-ratelimiter>><<google-cloud>>.

Rate-limiting strategies which discard traffic in exceeds of an allowed rate (e.g. with 429 http responses) are referred to as _traffic policing_ strategies, and strategies which instead defer requests until they conform to a tolerable request rate (e.g. by enqueuing requests for asynchronous fulfillment) are referred to as _traffic shaping_ strategies <<rfc-2475>><<wikipedia-traffic-policing>>.

There are multiple algorithms for rate-limiting (see <<google-cloud>><<wikipedia-ratelimiting>>), some of which we will explore below.

== Token Bucket (Leaky Bucket) Algorithm

NOTE: There are two implementations of the leaky bucket: one used to police traffic, and one used to shape it. The traffic-shaping version is conceptually the same as token bucket, described below. These amount to different mental models for the same algorithm. <<wikipedia-token-bucket>>

The token bucket algorithm is used to police or shape traffic to conform to a desired level of "burstiness" and average request rate <<science-direct>><<wikipedia-token-bucket>>. This strategy limits each request by the resources (e.g. database connections, CPU time) it requires such that overall traffic is limited on the basis of available system capacity rather than simple volume of requests <<google-cloud>>.

The algorithm is based on the analogy of a bucket filled with tokens. Requesters must retrieve some number of tokens from the bucket based on the capacity they wish to utilize. If the bucket has enough tokens, the requester removes the tokens and computes a response. If the bucket has too few tokens, the request is rejected or enqueued for later computation depending on implementation. At a fixed rate, tokens are added back to the bucket, but any excess tokens beyond the bucket's capacity are simply discarded <<google-cloud>><<cisco-token-bucket>><<wikipedia-token-bucket>>. The capacity of the bucket therefore determines the maximum instantaneous capacity that may be utilized by requests, and the rate at which tokens are added governs the long-term average utilization <<cisco-token-bucket>><<wikipedia-token-bucket>><<science-direct>>.

In telecommunications contexts, token buckets are often used to limit the average rate at which bits flow into a network with limited bandwidth <<cisco-token-bucket>><<wikipedia-token-bucket>><<science-direct>>. The literature describes such a token bucket algorithm with the equation `CIR = CBS / T` <<science-direct>>, where:

* `CIR`, the _committed information rate_ or _mean rate_, is the average rate at which bits are forwarded to the network;
* `CBS`, the _committed burst size_, is the maximum volume of bits submitted simultaneously during the time period `T`; and
* `T` is the maximum duration of a burst during which the network is fully utilized.

The capacity of a token bucket is represented by `CBS`; clients of the bucket can never request more than `CBS` tokens at a single time. Tokens are returned to the bucket at the rate `CIR`, which by definition means all `CBS` tokens are replenished every interval `T`. Therefore in the worst case, clients can request at most `CBS` tokens every interval `T`, which over the long-term conforms to the desired average request rate `CIR`.

== Examples

See the following examples:

- link:examples/rate-limiter/general/example.html[General behavior]
- link:examples/rate-limiter/token-bucket/example.html[Token buckets]
